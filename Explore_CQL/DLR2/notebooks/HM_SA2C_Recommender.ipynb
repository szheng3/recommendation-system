{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szheng3/recommendation-system/blob/main/Explore_CQL/DLR2/HM_SA2C_Recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ltyWBrP0cnH"
      },
      "source": [
        "# SA2C-SASRec Recommender on H&M Data with Conservative Q-learning\n",
        "\n",
        "In this notebook we train an SA2C_SASrec model without and with CQL to train for recommendation on H&M dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEz3djcf0w7C"
      },
      "source": [
        "1. Clone the git repository containing all the source code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vgK2r88Mmkd",
        "outputId": "e1c45511-508d-42cd-ccd8-614c46bd52cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'recommendation-system'...\n",
            "remote: Enumerating objects: 1163, done.\u001b[K\n",
            "remote: Counting objects: 100% (430/430), done.\u001b[K\n",
            "remote: Compressing objects: 100% (260/260), done.\u001b[K\n",
            "remote: Total 1163 (delta 219), reused 325 (delta 162), pack-reused 733\u001b[K\n",
            "Receiving objects: 100% (1163/1163), 10.55 MiB | 15.96 MiB/s, done.\n",
            "Resolving deltas: 100% (565/565), done.\n"
          ]
        }
      ],
      "source": [
        "# cloning the git repository\n",
        "!git clone https://github.com/szheng3/recommendation-system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8WLc6n_04S8"
      },
      "source": [
        "2. Install the missing libraries (specifically for google colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY3pkxR1AsDW",
        "outputId": "c732f949-fb95-4e1c-dbcb-4f82d4cf0c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting trfl\n",
            "  Downloading trfl-1.2.0-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 kB\u001b[0m \u001b[31m916.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from trfl) (1.4.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from trfl) (1.14.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from trfl) (0.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from trfl) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from trfl) (1.16.0)\n",
            "Installing collected packages: trfl\n",
            "Successfully installed trfl-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# installing missing libraries (specifically for google colab)\n",
        "!pip install trfl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WblRLpti1A4h"
      },
      "source": [
        "3. Download the H&M Dataset transaction log file from AWS S3 bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA7SMHfSMiHI",
        "outputId": "78c1ac99-f761-4915-90c1-b1aa1c1ecca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-02 20:26:57--  https://aipi590.s3.amazonaws.com/transactions_train.csv\n",
            "Resolving aipi590.s3.amazonaws.com (aipi590.s3.amazonaws.com)... 52.217.93.100, 54.231.199.217, 52.217.47.180, ...\n",
            "Connecting to aipi590.s3.amazonaws.com (aipi590.s3.amazonaws.com)|52.217.93.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3488002253 (3.2G) [text/csv]\n",
            "Saving to: ‘/content/recommendation-system/Explore_CQL/Data/HM_data/transactions_train.csv’\n",
            "\n",
            "transactions_train. 100%[===================>]   3.25G  27.6MB/s    in 76s     \n",
            "\n",
            "2023-05-02 20:28:13 (43.6 MB/s) - ‘/content/recommendation-system/Explore_CQL/Data/HM_data/transactions_train.csv’ saved [3488002253/3488002253]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# downloading the \"transactions_train.csv\" datafile from the H&M Dataset\n",
        "!wget https://aipi590.s3.amazonaws.com/transactions_train.csv -P \"/content/recommendation-system/Explore_CQL/Data/HM_data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB5AbLV31Pjg"
      },
      "source": [
        "4. Run the \"gen_replay_buffer.py\" script to pre-process data and generate replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-MWI_XktTHX",
        "outputId": "2e6e6c33-279d-4130-b2c4-c06d9cb2267a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start reading all transaction data ...\n",
            "Finish reading in 00:00:28\n",
            "\n",
            "Filter and save all valid sampled data\n",
            "Index(['timestamp', 'session_id', 'item_id', 'price', 'sales_channel_id'], dtype='object')\n",
            "\n",
            "Start counting popularity ...\n",
            "13040912it [08:16, 26272.28it/s]\n",
            "Popularity finished in 00:08:16\n",
            "\n",
            "Start spliting into train, val, test data ...\n",
            "\n",
            "           Generate Replay Buffer:\n",
            "                Total Session Size : 1245612\n",
            "                     Train:      871928 ids | 9124752 actions\n",
            "                     Validation: 249122 ids | 2611174 actions\n",
            "                     Test:       124562 ids | 1304986 actions\n",
            "                     \n",
            "                Random : True\n",
            "                Random Seed : 1234\n",
            "                Format : paper\n",
            "    \n",
            "                Total session id number : 1245612\n",
            "                Total item id number  : 96222\n",
            "    \n",
            "Generating training replay buffer\n",
            "100% 608701/608701 [09:45<00:00, 1038.91it/s]\n"
          ]
        }
      ],
      "source": [
        "# Executing the python script \"gen_replay_buffer\" to sample the given H&M dataset\n",
        "!python \"/content/recommendation-system/Explore_CQL/DLR2/src/gen_replay_buffer_HM.py\" --data=\"/content/recommendation-system/Explore_CQL/Data/HM_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNt1pnmh1aaI"
      },
      "source": [
        "5. Run script to begin training and evaluate the SASRec-SNQN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8Zk3vmWxMdV",
        "outputId": "e1cd5322-644e-49f3-8ac7-57fd40ec956c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-02 20:48:39.698824: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-05-02 20:48:39.752100: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-02 20:48:40.822309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Not using CQL loss\n",
            "2023-05-02 20:48:42.965890: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-02 20:48:43.651917: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-02 20:48:43.710890: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-02 20:48:43.752427: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "Not using CQL loss\n",
            "2023-05-02 20:48:46.748278: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-02 20:48:46.913445: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-02 20:48:46.975367: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-02 20:48:47.020525: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-02 20:49:16.865053: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-02 20:49:16.865148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38286 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n",
            "2023-05-02 20:49:16.969422: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
            "epoch 1\n",
            "2023-05-02 20:49:22.455136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
            "2023-05-02 20:49:24.539269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
            "the loss in 200th batch is: 11.541342\n",
            "the loss in 400th batch is: 11.274902\n",
            "the loss in 600th batch is: 11.180227\n",
            "the loss in 800th batch is: 11.282410\n",
            "the loss in 1000th batch is: 10.702670\n",
            "the loss in 1200th batch is: 10.698605\n",
            "the loss in 1400th batch is: 10.787557\n",
            "the loss in 1600th batch is: 10.763778\n",
            "the loss in 1800th batch is: 10.700291\n",
            "the loss in 2000th batch is: 10.547813\n",
            "the loss in 2200th batch is: 10.682529\n",
            "the loss in 2400th batch is: 10.692996\n",
            "the loss in 2600th batch is: 10.601251\n",
            "the loss in 2800th batch is: 10.785054\n",
            "the loss in 3000th batch is: 10.549043\n",
            "the loss in 3200th batch is: 10.382656\n",
            "the loss in 3400th batch is: 10.450514\n",
            "the loss in 3600th batch is: 10.709965\n",
            "the loss in 3800th batch is: 10.299559\n",
            "the loss in 4000th batch is: 10.486265\n",
            "the loss in 4200th batch is: 10.580622\n",
            "the loss in 4400th batch is: 10.329442\n",
            "the loss in 4600th batch is: 10.314987\n",
            "the loss in 4800th batch is: 10.426206\n",
            "the loss in 5000th batch is: 10.387191\n",
            "the loss in 5200th batch is: 10.437453\n",
            "the loss in 5400th batch is: 10.227304\n",
            "the loss in 5600th batch is: 10.375014\n",
            "the loss in 5800th batch is: 10.220689\n",
            "the loss in 6000th batch is: 10.278964\n",
            "the loss in 6200th batch is: 10.336676\n",
            "the loss in 6400th batch is: 10.230400\n",
            "the loss in 6600th batch is: 10.258953\n",
            "the loss in 6800th batch is: 10.228848\n",
            "the loss in 7000th batch is: 10.075352\n",
            "the loss in 7200th batch is: 10.225369\n",
            "the loss in 7400th batch is: 9.959764\n",
            "the loss in 7600th batch is: 10.117080\n",
            "the loss in 7800th batch is: 10.244329\n",
            "the loss in 8000th batch is: 10.146455\n",
            "\n",
            "Beginning evaluation...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/recommendation-system/Explore_CQL/DLR2/src/SA2C_v3.py\", line 540, in <module>\n",
            "    evaluate(sess)\n",
            "  File \"/content/recommendation-system/Explore_CQL/DLR2/src/SA2C_v3.py\", line 401, in evaluate\n",
            "    sorted_list=np.argsort(prediction)\n",
            "  File \"<__array_function__ internals>\", line 180, in argsort\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\", line 1120, in argsort\n",
            "    return _wrapfunc(a, 'argsort', axis=axis, kind=kind, order=order)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\", line 57, in _wrapfunc\n",
            "    return bound(*args, **kwds)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# Executing the python script \"SA2C_v3.py\" to train a SA2C-SASrec model on the sampled data and evaluate it on validation data using HR (hit rate) and NDCG metrics for different values of k\n",
        "!python \"/content/recommendation-system/Explore_CQL/DLR2/src/SA2C_v3_5.py\" --model=SASRec --data=\"/content/recommendation-system/Explore_CQL/Data/HM_data/\" --epoch=5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vZxRqVldJUwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Executing the python script \"SA2C_v3.py\" to train a SA2C-SASrec model on the sampled data with CQL and evaluate it on validation data using HR (hit rate) and NDCG metrics for different values of k\n",
        "!python \"/content/recommendation-system/Explore_CQL/DLR2/src/SA2C_v3_5.py\" --model=SASRec --CQL_alpha=1.0 --data=\"/content/recommendation-system/Explore_CQL/Data/HM_data/\" --epoch=5"
      ],
      "metadata": {
        "id": "M5NVOfakncxU",
        "outputId": "19a6abf7-b4b9-4248-cf96-34de0003ccb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-03 00:17:09.045701: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-05-03 00:17:09.098977: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-03 00:17:10.140498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: SA2C_v3.py\n",
            "       [-h]\n",
            "       [--epoch EPOCH]\n",
            "       [--data [DATA]]\n",
            "       [--batch_size BATCH_SIZE]\n",
            "       [--hidden_factor HIDDEN_FACTOR]\n",
            "       [--r_click R_CLICK]\n",
            "       [--r_buy R_BUY]\n",
            "       [--r_negative R_NEGATIVE]\n",
            "       [--lr LR]\n",
            "       [--discount DISCOUNT]\n",
            "       [--neg NEG]\n",
            "       [--weight WEIGHT]\n",
            "       [--smooth SMOOTH]\n",
            "       [--clip CLIP]\n",
            "       [--lr_2 LR_2]\n",
            "       [--model MODEL]\n",
            "       [--num_filters NUM_FILTERS]\n",
            "       [--filter_sizes [FILTER_SIZES]]\n",
            "       [--num_heads NUM_HEADS]\n",
            "       [--num_blocks NUM_BLOCKS]\n",
            "       [--dropout_rate DROPOUT_RATE]\n",
            "       [--CQL_alpha CQL_ALPHA]\n",
            "SA2C_v3.py: error: unrecognized arguments: use_CQL=True\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
