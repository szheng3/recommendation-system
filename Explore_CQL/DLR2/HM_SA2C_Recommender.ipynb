{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szheng3/recommendation-system/blob/main/Explore_CQL/DLR2/HM_SA2C_Recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ltyWBrP0cnH"
      },
      "source": [
        "# SA2C-SASRec Recommender on H&M Data with Conservative Q-learning\n",
        "\n",
        "In this notebook we train an SA2C_SASrec model without and with CQL to train for recommendation on H&M dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEz3djcf0w7C"
      },
      "source": [
        "1. Clone the git repository containing all the source code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vgK2r88Mmkd",
        "outputId": "37e67943-82e7-4e39-ad83-f3f503e2a221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'recommendation-system'...\n",
            "remote: Enumerating objects: 998, done.\u001b[K\n",
            "remote: Counting objects: 100% (265/265), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 998 (delta 125), reused 239 (delta 119), pack-reused 733\u001b[K\n",
            "Receiving objects: 100% (998/998), 8.57 MiB | 16.62 MiB/s, done.\n",
            "Resolving deltas: 100% (471/471), done.\n"
          ]
        }
      ],
      "source": [
        "# cloning the git repository\n",
        "!git clone -b cql https://github.com/szheng3/recommendation-system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8WLc6n_04S8"
      },
      "source": [
        "2. Install the missing libraries (specifically for google colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY3pkxR1AsDW",
        "outputId": "b293e20c-9424-4871-acfe-13a03ed0f99b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting trfl\n",
            "  Downloading trfl-1.2.0-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from trfl) (1.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from trfl) (0.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from trfl) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from trfl) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from trfl) (1.14.1)\n",
            "Installing collected packages: trfl\n",
            "Successfully installed trfl-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# installing missing libraries (specifically for google colab)\n",
        "!pip install trfl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WblRLpti1A4h"
      },
      "source": [
        "3. Download the H&M Dataset transaction log file from AWS S3 bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA7SMHfSMiHI",
        "outputId": "635e5464-2d7f-470b-a899-c1b5e1698341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-02 02:30:48--  https://aipi590.s3.amazonaws.com/transactions_train.csv\n",
            "Resolving aipi590.s3.amazonaws.com (aipi590.s3.amazonaws.com)... 52.217.75.44, 3.5.29.254, 52.216.60.121, ...\n",
            "Connecting to aipi590.s3.amazonaws.com (aipi590.s3.amazonaws.com)|52.217.75.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3488002253 (3.2G) [text/csv]\n",
            "Saving to: ‘/content/recommendation-system/Explore_CQL/Data/HM_data/transactions_train.csv’\n",
            "\n",
            "transactions_train. 100%[===================>]   3.25G  13.1MB/s    in 4m 18s  \n",
            "\n",
            "2023-05-02 02:35:07 (12.9 MB/s) - ‘/content/recommendation-system/Explore_CQL/Data/HM_data/transactions_train.csv’ saved [3488002253/3488002253]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# downloading the \"transactions_train.csv\" datafile from the H&M Dataset\n",
        "!wget https://aipi590.s3.amazonaws.com/transactions_train.csv -P \"/content/recommendation-system/Explore_CQL/Data/HM_data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB5AbLV31Pjg"
      },
      "source": [
        "4. Run the \"gen_replay_buffer.py\" script to pre-process data and generate replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-MWI_XktTHX",
        "outputId": "1fd3219f-dbda-4f37-b60c-0a7b2e18e498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-02 02:37:41.763497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-02 02:37:42.672697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "Starting to pre-process data...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/recommendation-system/Explore_CQL/DLR2/src/gen_replay_buffer.py\", line 36, in <module>\n",
            "    event_df = pd.read_csv(os.path.join(DATA, 'events.csv'), header=0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 211, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 331, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 605, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1735, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\", line 856, in get_handle\n",
            "    handle = open(\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/recommendation-system/Explore_CQL/Data/HM_data/events.csv'\n"
          ]
        }
      ],
      "source": [
        "# Executing the python script \"gen_replay_buffer\" to sample the given H&M dataset\n",
        "!python \"/content/recommendation-system/Explore_CQL/DLR2/src/gen_replay_buffer.py\" --data=\"/content/recommendation-system/Explore_CQL/Data/HM_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNt1pnmh1aaI"
      },
      "source": [
        "5. Run script to begin training and evaluate the SASRec-SNQN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8Zk3vmWxMdV",
        "outputId": "92cb4799-0d1b-409a-bb25-1fdb3770dd36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "/content/recommenders_aipi590/DRL_Recommenders/Dataset_2_HM/src/SNQN_v2.py:188: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  self.seq = tf.compat.v1.layers.dropout(self.seq,\n",
            "/content/recommenders_aipi590/DRL_Recommenders/Dataset_2_HM/src/SASRecModules_v2.py:142: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  Q = tf.compat.v1.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n",
            "/content/recommenders_aipi590/DRL_Recommenders/Dataset_2_HM/src/SASRecModules_v2.py:143: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  K = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
            "/content/recommenders_aipi590/DRL_Recommenders/Dataset_2_HM/src/SASRecModules_v2.py:144: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  V = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n",
            "/content/recommenders_aipi590/DRL_Recommenders/Dataset_2_HM/src/SASRecModules_v2.py:184: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n",
            "/content/recommenders_aipi590/DRL_Recommenders/Dataset_2_HM/src/SASRecModules_v2.py:223: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.compat.v1.layers.conv1d(**params)\n",
            "/content/recommenders_aipi590/DRL_Recommenders/Dataset_2_HM/src/SASRecModules_v2.py:224: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n",
            "/content/recommenders_aipi590/DRL_Recommenders/Dataset_2_HM/src/SASRecModules_v2.py:228: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n",
            "  outputs = tf.compat.v1.layers.conv1d(**params)\n",
            "/content/recommenders_aipi590/DRL_Recommenders/Dataset_2_HM/src/SASRecModules_v2.py:229: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n",
            "  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n",
            "/content/recommenders_aipi590/DRL_Recommenders/Dataset_2_HM/src/SNQN_v2.py:216: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output1 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "/content/recommenders_aipi590/DRL_Recommenders/Dataset_2_HM/src/SNQN_v2.py:219: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.output2 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n",
            "2022-12-12 12:07:15.991484: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "\n",
            "********************* Training Started *********************\n",
            "\n",
            "Starting Epoch: 1\n",
            "Number of batches: 580\n",
            "Epoch 1 training completed.\n",
            "The loss after epoch 1 is: 13.455638885498047\n",
            "\n",
            "Starting Epoch: 2\n",
            "Number of batches: 580\n",
            "Epoch 2 training completed.\n",
            "The loss after epoch 2 is: 12.278966903686523\n",
            "\n",
            "Starting Epoch: 3\n",
            "Number of batches: 580\n",
            "Epoch 3 training completed.\n",
            "The loss after epoch 3 is: 11.628719329833984\n",
            "\n",
            "Starting Epoch: 4\n",
            "Number of batches: 580\n",
            "Epoch 4 training completed.\n",
            "The loss after epoch 4 is: 10.688684463500977\n",
            "\n",
            "Starting Epoch: 5\n",
            "Number of batches: 580\n",
            "Epoch 5 training completed.\n",
            "The loss after epoch 5 is: 10.170299530029297\n",
            "\n",
            "Starting Epoch: 6\n",
            "Number of batches: 580\n",
            "Epoch 6 training completed.\n",
            "The loss after epoch 6 is: 9.929465293884277\n",
            "\n",
            "Starting Epoch: 7\n",
            "Number of batches: 580\n",
            "Epoch 7 training completed.\n",
            "The loss after epoch 7 is: 8.787910461425781\n",
            "\n",
            "Starting Epoch: 8\n",
            "Number of batches: 580\n",
            "Epoch 8 training completed.\n",
            "The loss after epoch 8 is: 8.504495620727539\n",
            "\n",
            "Starting Epoch: 9\n",
            "Number of batches: 580\n",
            "Epoch 9 training completed.\n",
            "The loss after epoch 9 is: 8.340208053588867\n",
            "\n",
            "Starting Epoch: 10\n",
            "Number of batches: 580\n",
            "Epoch 10 training completed.\n",
            "The loss after epoch 10 is: 7.463860511779785\n",
            "\n",
            "Starting Epoch: 11\n",
            "Number of batches: 580\n",
            "Epoch 11 training completed.\n",
            "The loss after epoch 11 is: 7.591894626617432\n",
            "\n",
            "Starting Epoch: 12\n",
            "Number of batches: 580\n",
            "Epoch 12 training completed.\n",
            "The loss after epoch 12 is: 7.115472793579102\n",
            "\n",
            "Starting Epoch: 13\n",
            "Number of batches: 580\n",
            "Epoch 13 training completed.\n",
            "The loss after epoch 13 is: 6.816874980926514\n",
            "\n",
            "Starting Epoch: 14\n",
            "Number of batches: 580\n",
            "Epoch 14 training completed.\n",
            "The loss after epoch 14 is: 6.612104415893555\n",
            "\n",
            "Starting Epoch: 15\n",
            "Number of batches: 580\n",
            "Epoch 15 training completed.\n",
            "The loss after epoch 15 is: 6.295536518096924\n",
            "\n",
            "Starting Epoch: 16\n",
            "Number of batches: 580\n",
            "Epoch 16 training completed.\n",
            "The loss after epoch 16 is: 6.289534091949463\n",
            "\n",
            "Starting Epoch: 17\n",
            "Number of batches: 580\n",
            "Epoch 17 training completed.\n",
            "The loss after epoch 17 is: 5.988742828369141\n",
            "\n",
            "Starting Epoch: 18\n",
            "Number of batches: 580\n",
            "Epoch 18 training completed.\n",
            "The loss after epoch 18 is: 6.136661052703857\n",
            "\n",
            "Starting Epoch: 19\n",
            "Number of batches: 580\n",
            "Epoch 19 training completed.\n",
            "The loss after epoch 19 is: 5.589191913604736\n",
            "\n",
            "Starting Epoch: 20\n",
            "Number of batches: 580\n",
            "Epoch 20 training completed.\n",
            "The loss after epoch 20 is: 5.826780319213867\n",
            "\n",
            "Start evaluation with batch size 30\n",
            "tcmalloc: large alloc 1851318272 bytes == 0x8628e000 @  0x7f7e7ec891e7 0x7f7e494f114e 0x7f7e49549745 0x7f7e49549878 0x7f7e4958f597 0x7f7e495937dc 0x7f7e495e2a72 0x5aae14 0x5d8416 0x5630f5 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x5d8506 0x7f7e49533944 0x5d814d 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630 0x6426ae 0x644b78 0x64511c\n",
            "tcmalloc: large alloc 1614995456 bytes == 0xf482a000 @  0x7f7e7ec891e7 0x7f7e494f114e 0x7f7e49549745 0x7f7e49549878 0x7f7e4958f597 0x7f7e495937dc 0x7f7e495e2a72 0x5aae14 0x5d8416 0x5630f5 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x5d8506 0x7f7e49533944 0x5d814d 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630 0x6426ae 0x644b78 0x64511c\n",
            "tcmalloc: large alloc 1684275200 bytes == 0x8628e000 @  0x7f7e7ec891e7 0x7f7e494f114e 0x7f7e49549745 0x7f7e49549878 0x7f7e4958f597 0x7f7e495937dc 0x7f7e495e2a72 0x5aae14 0x5d8416 0x5630f5 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x5d8506 0x7f7e49533944 0x5d814d 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630 0x6426ae 0x644b78 0x64511c\n",
            "tcmalloc: large alloc 1691205632 bytes == 0x157d9e000 @  0x7f7e7ec891e7 0x7f7e494f114e 0x7f7e49549745 0x7f7e49549878 0x7f7e4958f597 0x7f7e495937dc 0x7f7e495e2a72 0x5aae14 0x5d8416 0x5630f5 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x5d8506 0x7f7e49533944 0x5d814d 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x4997c7 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630 0x6426ae 0x644b78 0x64511c\n",
            "#############################################################\n",
            "total clicks: 0, total purchase:41673\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 3015.000000\n",
            "purchase hr and ndcg @5 : 0.024116, 0.015475\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 4716.000000\n",
            "purchase hr and ndcg @10 : 0.037722, 0.019848\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 5688.000000\n",
            "purchase hr and ndcg @15 : 0.045497, 0.021906\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 6432.000000\n",
            "purchase hr and ndcg @20 : 0.051448, 0.023314\n",
            "#############################################################\n"
          ]
        }
      ],
      "source": [
        "# Executing the python script \"SA2C_v3.py\" to train a SA2C-SASrec model on the sampled data and evaluate it on validation data using HR (hit rate) and NDCG metrics for different values of k\n",
        "!python \"/content/recommendation-system/Explore_CQL/DLR2/src/SA2C_v3.py\" --model=SASRec --data=\"/content/recommendation-system/Explore_CQL/Data/HM_Data/\" --epoch=10"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}