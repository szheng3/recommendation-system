{"cells":[{"cell_type":"markdown","metadata":{"id":"QPKSPupt1XeS"},"source":["# Using generated CQL Loss from TRFL Package"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOyRGcgN6cdv"},"outputs":[],"source":["!pip install git+\"https://github.com/abonafede/trfl\"\n","!git clone https://github.com/szheng3/recommendation-system.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a67pqjNfF6bb"},"outputs":[],"source":["#!pip install -r '/content/recommendation-system/Explore_CQL/requirements.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bhBxzuZ6sZR"},"outputs":[],"source":["!wget https://aipi590.s3.amazonaws.com/events.csv -P '/content/recommendation-system/Explore_CQL/Data/RR_data'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TprxYm5w-UT7","outputId":"799f5046-8167-4e8e-82f7-67732ff4faad"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-05-02 10:14:05.224855: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-02 10:14:06.866919: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2023-05-02 10:14:06.867046: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2023-05-02 10:14:06.867065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","\n","Starting to pre-process data...\n","\n","Sorting and pickling data...\n","\n","Splitting data into train, validation, and test sets...\n","\n","Pickling train, validation, and test sets...\n","\n","Calculating item popularity and storing as dictionary...\n","\n","Generating replay buffer from train set...\n","\n","Pickling replay buffer...\n","\n","Pickling data statistics...\n","\n","Script completed successfully!\n"]}],"source":["!python '/content/recommendation-system/Explore_CQL/DRL2/src/gen_replay_buffer.py' --data='/content/recommendation-system/Explore_CQL/Data/RR_data'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mZqTP5arnNtS"},"outputs":[],"source":["!tf_upgrade_v2 \\\n","  --infile 'aipi540-s23/ajb146/Explore_CQL/DRL2/src/SA2C_vAndrew.py' \\\n","  --outfile 'aipi540-s23/ajb146/Explore_CQL/DRL2/src/SA2C_vAndrew.py' \\\n","  --reportfile report_SA2C.txt\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GU9Ma5dz1g20"},"source":["# NO CQL Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2jjqlOt--UT7","outputId":"0826ce08-818f-4ba4-fc91-72e12723c9e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-05-02 00:10:01.896187: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-02 00:10:05.049708: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2023-05-02 00:10:05.049799: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2023-05-02 00:10:05.049817: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","./src/SA2C_vAndrew.py:188: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","  self.seq = tf.compat.v1.layers.dropout(self.seq,\n","2023-05-02 00:10:12.308176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 00:10:12.342949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 00:10:12.343199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:142: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  Q = tf.compat.v1.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:143: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  K = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:144: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  V = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:184: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:223: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n","  outputs = tf.compat.v1.layers.conv1d(**params)\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:224: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:228: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n","  outputs = tf.compat.v1.layers.conv1d(**params)\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:229: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n","./src/SA2C_vAndrew.py:216: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  self.output1 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n","./src/SA2C_vAndrew.py:219: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  self.output2= tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n","2023-05-02 00:10:23.446673: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-02 00:10:23.447248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 00:10:23.447564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 00:10:23.447746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 00:10:24.066892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 00:10:24.067156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 00:10:24.067332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 00:10:24.067489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22334 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:13:00.0, compute capability: 8.6\n","2023-05-02 00:10:24.175960: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n","2023-05-02 00:10:26.304130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n","2023-05-02 00:10:26.463064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8301\n","2023-05-02 00:10:27.686001: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n","the loss in 200th batch is: 10.865400\n","the loss in 400th batch is: 10.611997\n","the loss in 600th batch is: 10.572788\n","the loss in 800th batch is: 10.363742\n","the loss in 1000th batch is: 10.408920\n","the loss in 1200th batch is: 10.343833\n","the loss in 2600th batch is: 9.297973\n","the loss in 2800th batch is: 9.106581\n","the loss in 3000th batch is: 9.142845\n","the loss in 3200th batch is: 9.057177\n","the loss in 3400th batch is: 8.891904\n","the loss in 3600th batch is: 8.815584\n","the loss in 3800th batch is: 8.705700\n","the loss in 4000th batch is: 8.779776\n","#############################################################\n","total clicks: 117015, total purchase:5578\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 6432.800000\n","clicks hr ndcg @ 5 : 0.183130, 0.144340\n","purchase hr and ndcg @5 : 0.384905, 0.331333\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 7384.800000\n","clicks hr ndcg @ 10 : 0.215434, 0.154804\n","purchase hr and ndcg @10 : 0.420043, 0.342718\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 7912.000000\n","clicks hr ndcg @ 15 : 0.233133, 0.159494\n","purchase hr and ndcg @15 : 0.440301, 0.348097\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 8284.200000\n","clicks hr ndcg @ 20 : 0.246003, 0.162535\n","purchase hr and ndcg @20 : 0.453030, 0.351099\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 0.032355, 0.095754\n","#############################################################\n","the loss in 4200th batch is: 8.532200\n","the loss in 4400th batch is: 8.527349\n","the loss in 4600th batch is: 8.508130\n","the loss in 4800th batch is: 8.390391\n","the loss in 5000th batch is: 8.275447\n","the loss in 5200th batch is: 8.031124\n","the loss in 5400th batch is: 7.920357\n","the loss in 5600th batch is: 7.877966\n","the loss in 5800th batch is: 7.852716\n","the loss in 6000th batch is: 7.578201\n","the loss in 6200th batch is: 7.851808\n","the loss in 6400th batch is: 7.608829\n","the loss in 6600th batch is: 7.773365\n","the loss in 6800th batch is: 7.944873\n","the loss in 7000th batch is: 7.792900\n","the loss in 7200th batch is: 7.688116\n","the loss in 7400th batch is: 7.557940\n","the loss in 7600th batch is: 7.516981\n","the loss in 7800th batch is: 7.399698\n","the loss in 8000th batch is: 7.177483\n","#############################################################\n","total clicks: 117015, total purchase:5578\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 8555.600000\n","clicks hr ndcg @ 5 : 0.245379, 0.192267\n","purchase hr and ndcg @5 : 0.504303, 0.429579\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 9856.600000\n","clicks hr ndcg @ 10 : 0.290117, 0.206757\n","purchase hr and ndcg @10 : 0.549839, 0.444504\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 10567.000000\n","clicks hr ndcg @ 15 : 0.314404, 0.213191\n","purchase hr and ndcg @15 : 0.575296, 0.451271\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 11040.200000\n","clicks hr ndcg @ 20 : 0.331248, 0.217173\n","purchase hr and ndcg @20 : 0.589459, 0.454617\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 0.059990, 0.166792\n","#############################################################\n","the loss in 8200th batch is: 7.766286\n","the loss in 8400th batch is: 7.537183\n","the loss in 8600th batch is: 7.021205\n","the loss in 8800th batch is: 7.074050\n","the loss in 9000th batch is: 6.943969\n","the loss in 9200th batch is: 7.125570\n","the loss in 9400th batch is: 6.960657\n","the loss in 9600th batch is: 7.157137\n","the loss in 9800th batch is: 6.998394\n","the loss in 10000th batch is: 6.772305\n","the loss in 10200th batch is: 6.442751\n","the loss in 10400th batch is: 6.796587\n","the loss in 10600th batch is: 7.128781\n","the loss in 10800th batch is: 7.012331\n","the loss in 11000th batch is: 6.547211\n","the loss in 11200th batch is: 6.644774\n","the loss in 11400th batch is: 6.479117\n","the loss in 11600th batch is: 6.646960\n","the loss in 11800th batch is: 6.468735\n","the loss in 12000th batch is: 6.404976\n","#############################################################\n","total clicks: 117015, total purchase:5578\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 9356.800000\n","clicks hr ndcg @ 5 : 0.266709, 0.207932\n","purchase hr and ndcg @5 : 0.558444, 0.473161\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 10827.000000\n","clicks hr ndcg @ 10 : 0.317609, 0.224445\n","purchase hr and ndcg @10 : 0.608462, 0.489480\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 11639.600000\n","clicks hr ndcg @ 15 : 0.346605, 0.232127\n","purchase hr and ndcg @15 : 0.632485, 0.495830\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 12179.000000\n","clicks hr ndcg @ 20 : 0.365808, 0.236668\n","purchase hr and ndcg @20 : 0.648620, 0.499659\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 0.076633, 0.213229\n","#############################################################\n","the loss in 12200th batch is: 6.347189\n","the loss in 12400th batch is: 6.792012\n","the loss in 12600th batch is: 6.338804\n","the loss in 12800th batch is: 6.462991\n","the loss in 13000th batch is: 6.316225\n","the loss in 13200th batch is: 6.453479\n","the loss in 13400th batch is: 6.109516\n","the loss in 13600th batch is: 6.202642\n","the loss in 13800th batch is: 6.347593\n","the loss in 14000th batch is: 6.473789\n","the loss in 14200th batch is: 6.454428\n","the loss in 14400th batch is: 6.395012\n","the loss in 14600th batch is: 6.021441\n","the loss in 14800th batch is: 6.319549\n","the loss in 15000th batch is: 6.065107\n","the loss in 15200th batch is: 1.168728\n","the loss in 15400th batch is: 1.093869\n","the loss in 15600th batch is: 1.220881\n","the loss in 15800th batch is: 1.143300\n","the loss in 16000th batch is: 1.067131\n","#############################################################\n","total clicks: 117015, total purchase:5578\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 8661.200000\n","clicks hr ndcg @ 5 : 0.249421, 0.192658\n","purchase hr and ndcg @5 : 0.506275, 0.426269\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 10166.200000\n","clicks hr ndcg @ 10 : 0.300013, 0.209053\n","purchase hr and ndcg @10 : 0.563822, 0.444971\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 10937.600000\n","clicks hr ndcg @ 15 : 0.326821, 0.216157\n","purchase hr and ndcg @15 : 0.589638, 0.451811\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 11470.800000\n","clicks hr ndcg @ 20 : 0.345246, 0.220510\n","purchase hr and ndcg @20 : 0.607924, 0.456135\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 0.061552, 0.172696\n","#############################################################\n","the loss in 16200th batch is: 1.165384\n","the loss in 16400th batch is: 1.074463\n","the loss in 16600th batch is: 1.078254\n","the loss in 16800th batch is: 1.098013\n","the loss in 17000th batch is: 0.997306\n","the loss in 17200th batch is: 0.986160\n","the loss in 17400th batch is: 1.035664\n","the loss in 17600th batch is: 1.080574\n","the loss in 17800th batch is: 1.031801\n","the loss in 18000th batch is: 0.974274\n","the loss in 18200th batch is: 0.967100\n","the loss in 18400th batch is: 1.005784\n","the loss in 18600th batch is: 0.931248\n","the loss in 18800th batch is: 1.102920\n","the loss in 19000th batch is: 0.942275\n","the loss in 19200th batch is: 0.854084\n","the loss in 19400th batch is: 0.944230\n","the loss in 19600th batch is: 0.919352\n","the loss in 19800th batch is: 0.837182\n","the loss in 20000th batch is: 0.909107\n","#############################################################\n","total clicks: 117015, total purchase:5578\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 9159.000000\n","clicks hr ndcg @ 5 : 0.262701, 0.205322\n","purchase hr and ndcg @5 : 0.539799, 0.470911\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 10572.600000\n","clicks hr ndcg @ 10 : 0.312122, 0.221343\n","purchase hr and ndcg @10 : 0.585873, 0.485998\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 11340.000000\n","clicks hr ndcg @ 15 : 0.339486, 0.228586\n","purchase hr and ndcg @15 : 0.608641, 0.492070\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 11841.400000\n","clicks hr ndcg @ 20 : 0.357279, 0.232791\n","purchase hr and ndcg @20 : 0.623880, 0.495676\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 0.065276, 0.190078\n","#############################################################\n","the loss in 20200th batch is: 0.962900\n","the loss in 20400th batch is: 0.878487\n","the loss in 20600th batch is: 0.801938\n","the loss in 20800th batch is: 0.843696\n","the loss in 21000th batch is: 0.946023\n","the loss in 21200th batch is: 0.952729\n","the loss in 21400th batch is: 0.933284\n","the loss in 21600th batch is: 0.904481\n","the loss in 21800th batch is: 0.924585\n","the loss in 22000th batch is: 0.842341\n","the loss in 22200th batch is: 0.887090\n","the loss in 22400th batch is: 0.837914\n","the loss in 22600th batch is: 0.886317\n","the loss in 22800th batch is: 0.823051\n","the loss in 23000th batch is: 0.804252\n","the loss in 23200th batch is: 0.850330\n","the loss in 23400th batch is: 0.787838\n","the loss in 23600th batch is: 0.772705\n","the loss in 23800th batch is: 0.873952\n","the loss in 24000th batch is: 0.822679\n","#############################################################\n","total clicks: 117015, total purchase:5578\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 9502.200000\n","clicks hr ndcg @ 5 : 0.272281, 0.214286\n","purchase hr and ndcg @5 : 0.561133, 0.492158\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 10924.000000\n","clicks hr ndcg @ 10 : 0.322437, 0.230548\n","purchase hr and ndcg @10 : 0.605593, 0.506587\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 11681.400000\n","clicks hr ndcg @ 15 : 0.349887, 0.237826\n","purchase hr and ndcg @15 : 0.626210, 0.512057\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 12186.400000\n","clicks hr ndcg @ 20 : 0.368047, 0.242118\n","purchase hr and ndcg @20 : 0.640552, 0.515440\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 0.072216, 0.209509\n","#############################################################\n","the loss in 24200th batch is: 0.788372\n","the loss in 24400th batch is: 0.811574\n","the loss in 24600th batch is: 0.810621\n","the loss in 24800th batch is: 0.766540\n","the loss in 25000th batch is: 0.750530\n","the loss in 25200th batch is: 0.734280\n","the loss in 25400th batch is: 0.852187\n","the loss in 25600th batch is: 0.777574\n","the loss in 25800th batch is: 0.730886\n","the loss in 26000th batch is: 0.742464\n","the loss in 26200th batch is: 0.759579\n","the loss in 26400th batch is: 0.748619\n","the loss in 26600th batch is: 0.695085\n","the loss in 26800th batch is: 0.729908\n","the loss in 27000th batch is: 0.730415\n","the loss in 27200th batch is: 0.696898\n","the loss in 27400th batch is: 0.727569\n","the loss in 27600th batch is: 0.720594\n","the loss in 27800th batch is: 0.763993\n","the loss in 28000th batch is: 0.707181\n","the loss in 28600th batch is: 0.662650\n","the loss in 28800th batch is: 0.726356\n","the loss in 29000th batch is: 0.703149\n","the loss in 29200th batch is: 0.689697\n","the loss in 29400th batch is: 0.706349\n","the loss in 29600th batch is: 0.669863\n","the loss in 29800th batch is: 0.656358\n","the loss in 30000th batch is: 0.772345\n","the loss in 30200th batch is: 0.745664\n","the loss in 30400th batch is: 0.686478\n","the loss in 30600th batch is: 0.682246\n","the loss in 30800th batch is: 0.740541\n","the loss in 31000th batch is: 0.732637\n","the loss in 31200th batch is: 0.758810\n","the loss in 31400th batch is: 0.720308\n","the loss in 31600th batch is: 0.654122\n","the loss in 31800th batch is: 0.649249\n","the loss in 32000th batch is: 0.736113\n","the loss in 32800th batch is: 0.671829\n","the loss in 33000th batch is: 0.713415\n","the loss in 33200th batch is: 0.680382\n","the loss in 33400th batch is: 0.741778\n","the loss in 33600th batch is: 0.640472\n","the loss in 33800th batch is: 0.582804\n","the loss in 34000th batch is: 0.585874\n","the loss in 34200th batch is: 0.740247\n","the loss in 34400th batch is: 0.576381\n","the loss in 34600th batch is: 0.701741\n","the loss in 34800th batch is: 0.520096\n","the loss in 35000th batch is: 0.626326\n","the loss in 35200th batch is: 0.685992\n","the loss in 35400th batch is: 0.670582\n","the loss in 35600th batch is: 0.707867\n","the loss in 35800th batch is: 0.635547\n","the loss in 36000th batch is: 0.646289\n","#############################################################\n","total clicks: 117015, total purchase:5578\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 9982.400000\n","clicks hr ndcg @ 5 : 0.284938, 0.224023\n","purchase hr and ndcg @5 : 0.594120, 0.514009\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 11444.000000\n","clicks hr ndcg @ 10 : 0.336581, 0.240769\n","purchase hr and ndcg @10 : 0.639477, 0.528691\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 12225.200000\n","clicks hr ndcg @ 15 : 0.365389, 0.248398\n","purchase hr and ndcg @15 : 0.658659, 0.533781\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 12736.200000\n","clicks hr ndcg @ 20 : 0.384062, 0.252810\n","purchase hr and ndcg @20 : 0.671925, 0.536919\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 0.084969, 0.244224\n","#############################################################\n","the loss in 36200th batch is: 0.596861\n","the loss in 36400th batch is: 0.617203\n","the loss in 36600th batch is: 0.583185\n","the loss in 36800th batch is: 0.566448\n","the loss in 37000th batch is: 0.635610\n","the loss in 37200th batch is: 0.577976\n","the loss in 37400th batch is: 0.684878\n","the loss in 37600th batch is: 0.613896\n","the loss in 37800th batch is: 0.590497\n","the loss in 38000th batch is: 0.676065\n","the loss in 38200th batch is: 0.636130\n","the loss in 38400th batch is: 0.625064\n"]}],"source":["!python \"/content/recommendation-system/Explore_CQL/DRL2/src/SA2C_vAndrew.py\" --model=SASRec --epoch=10 --data='/content/recommendation-system/Explore_CQL/Data/RR_data'"]},{"cell_type":"markdown","metadata":{"id":"px8bBUoo1jo7"},"source":["# With CQL Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1SJ9Ux0-1owk","scrolled":false,"outputId":"539a642c-7f73-4214-be51-f3af8a1e148d"},"outputs":[{"name":"stdout","output_type":"stream","text":["2023-05-02 10:23:18.369577: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-02 10:23:20.124421: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2023-05-02 10:23:20.124523: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2023-05-02 10:23:20.124540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","./src/SA2C_vAndrew.py:188: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","  self.seq = tf.compat.v1.layers.dropout(self.seq,\n","2023-05-02 10:23:23.892068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 10:23:23.939132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 10:23:23.939403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:142: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  Q = tf.compat.v1.layers.dense(queries, num_units, activation=None) # (N, T_q, C)\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:143: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  K = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:144: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  V = tf.compat.v1.layers.dense(keys, num_units, activation=None) # (N, T_k, C)\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:184: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:223: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n","  outputs = tf.compat.v1.layers.conv1d(**params)\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:224: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:228: UserWarning: `tf.layers.conv1d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv1D` instead.\n","  outputs = tf.compat.v1.layers.conv1d(**params)\n","/hpc/group/aipi540-s23/ajb146/Explore_CQL/DLR2/src/SASRecModules_v2.py:229: UserWarning: `tf.layers.dropout` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dropout` instead.\n","  outputs = tf.compat.v1.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(value=is_training))\n","./src/SA2C_vAndrew.py:216: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  self.output1 = tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n","./src/SA2C_vAndrew.py:219: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  self.output2= tf.compat.v1.layers.dense(self.states_hidden, self.item_num,\n","2023-05-02 10:23:33.088416: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-02 10:23:33.088857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 10:23:33.089099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 10:23:33.089230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 10:23:33.548474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 10:23:33.548696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 10:23:33.548848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2023-05-02 10:23:33.548985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22334 MB memory:  -> device: 0, name: NVIDIA RTX A5000, pci bus id: 0000:13:00.0, compute capability: 8.6\n","2023-05-02 10:23:33.644605: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n","2023-05-02 10:23:35.405325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n","2023-05-02 10:23:35.546841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8301\n","2023-05-02 10:23:36.620934: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n","the loss in 200th batch is: 24.988255\n","the loss in 400th batch is: 15.327339\n","the loss in 600th batch is: 14.656082\n","the loss in 800th batch is: 13.745211\n","the loss in 1000th batch is: 13.652288\n","the loss in 1200th batch is: 12.961393\n","the loss in 1400th batch is: 12.912883\n","the loss in 1600th batch is: 12.731262\n","the loss in 1800th batch is: 12.068395\n","the loss in 2000th batch is: 12.629667\n","the loss in 2200th batch is: 12.717131\n","the loss in 2400th batch is: 12.417440\n","the loss in 2600th batch is: 12.387459\n","the loss in 2800th batch is: 12.516395\n","the loss in 3000th batch is: 12.402467\n","the loss in 3200th batch is: 12.275403\n","the loss in 3400th batch is: 11.761618\n","the loss in 3600th batch is: 12.402194\n","the loss in 3800th batch is: 11.958083\n","the loss in 4000th batch is: 11.772151\n","#############################################################\n","total clicks: 117015, total purchase:5578\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 255.000000\n","clicks hr ndcg @ 5 : 0.007307, 0.005118\n","purchase hr and ndcg @5 : 0.015059, 0.011747\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 370.800000\n","clicks hr ndcg @ 10 : 0.010973, 0.006286\n","purchase hr and ndcg @10 : 0.020437, 0.013424\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 475.600000\n","clicks hr ndcg @ 15 : 0.014255, 0.007148\n","purchase hr and ndcg @15 : 0.025457, 0.014748\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 572.600000\n","clicks hr ndcg @ 20 : 0.016989, 0.007795\n","purchase hr and ndcg @20 : 0.031373, 0.016135\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 0.000204, 0.000474\n","#############################################################\n","the loss in 4200th batch is: 11.573022\n","the loss in 4400th batch is: 11.501331\n","the loss in 4600th batch is: 11.917049\n","the loss in 4800th batch is: 11.502090\n","the loss in 5000th batch is: 11.873547\n","the loss in 5200th batch is: 11.627247\n","the loss in 5400th batch is: 11.934177\n","the loss in 5600th batch is: 11.639948\n","the loss in 5800th batch is: 11.445272\n","the loss in 6000th batch is: 12.239387\n","the loss in 6200th batch is: 12.061311\n","the loss in 6400th batch is: 11.374657\n","the loss in 6600th batch is: 11.420826\n","the loss in 6800th batch is: 10.968233\n","the loss in 7000th batch is: 11.415416\n","the loss in 7200th batch is: 11.369080\n","the loss in 7400th batch is: 11.014803\n","the loss in 7600th batch is: 10.742500\n","the loss in 7800th batch is: 10.564124\n","the loss in 8000th batch is: 10.541290\n","#############################################################\n","total clicks: 117015, total purchase:5578\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 2776.400000\n","clicks hr ndcg @ 5 : 0.077529, 0.059379\n","purchase hr and ndcg @5 : 0.172463, 0.137439\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 3360.400000\n","clicks hr ndcg @ 10 : 0.094834, 0.064948\n","purchase hr and ndcg @10 : 0.204554, 0.147742\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 3717.000000\n","clicks hr ndcg @ 15 : 0.106439, 0.068024\n","purchase hr and ndcg @15 : 0.219792, 0.151784\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 4038.200000\n","clicks hr ndcg @ 20 : 0.115934, 0.070263\n","purchase hr and ndcg @20 : 0.237540, 0.155988\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 0.006946, 0.022481\n","#############################################################\n","the loss in 8200th batch is: 10.528953\n","the loss in 8400th batch is: 10.648894\n","the loss in 8600th batch is: 11.211742\n","the loss in 8800th batch is: 11.247123\n","the loss in 9000th batch is: 11.064640\n","the loss in 9200th batch is: 11.227823\n","the loss in 9400th batch is: 11.559674\n","the loss in 9600th batch is: 10.764091\n","the loss in 9800th batch is: 10.064619\n","the loss in 10000th batch is: 9.595695\n","the loss in 10200th batch is: 11.246606\n","the loss in 10400th batch is: 10.205396\n","the loss in 10600th batch is: 10.010428\n","the loss in 10800th batch is: 10.115846\n","the loss in 11000th batch is: 10.885762\n","the loss in 11200th batch is: 9.926261\n","the loss in 11400th batch is: 10.670097\n","the loss in 11600th batch is: 9.725800\n","the loss in 11800th batch is: 10.425688\n","the loss in 12000th batch is: 9.636754\n","#############################################################\n","total clicks: 117015, total purchase:5578\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 5183.200000\n","clicks hr ndcg @ 5 : 0.147639, 0.114780\n","purchase hr and ndcg @5 : 0.309788, 0.259135\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 6193.800000\n","clicks hr ndcg @ 10 : 0.179242, 0.124986\n","purchase hr and ndcg @10 : 0.358372, 0.274954\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 6776.800000\n","clicks hr ndcg @ 15 : 0.198043, 0.129968\n","purchase hr and ndcg @15 : 0.384009, 0.281745\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 7205.200000\n","clicks hr ndcg @ 20 : 0.211862, 0.133232\n","purchase hr and ndcg @20 : 0.402833, 0.286176\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 0.022103, 0.064009\n","#############################################################\n","the loss in 12200th batch is: 9.337621\n","the loss in 12400th batch is: 10.649553\n","the loss in 12600th batch is: 9.707878\n","the loss in 12800th batch is: 10.651606\n","the loss in 13000th batch is: 10.149637\n","the loss in 13200th batch is: 10.367666\n","the loss in 13400th batch is: 10.220328\n","the loss in 13600th batch is: 10.605126\n","the loss in 13800th batch is: 9.883905\n","the loss in 14000th batch is: 10.193233\n","the loss in 14200th batch is: 9.858451\n","the loss in 14400th batch is: 9.943996\n","the loss in 14600th batch is: 10.196729\n","the loss in 14800th batch is: 9.086062\n","the loss in 15000th batch is: 9.667915\n","the loss in 15200th batch is: 2.469940\n","the loss in 15400th batch is: 2.448939\n","the loss in 15600th batch is: 2.867938\n","the loss in 15800th batch is: 2.538107\n","the loss in 16000th batch is: 2.678746\n","#############################################################\n","total clicks: 117015, total purchase:5578\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 5: 5870.000000\n","clicks hr ndcg @ 5 : 0.167500, 0.129722\n","purchase hr and ndcg @5 : 0.349588, 0.293096\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 10: 6879.600000\n","clicks hr ndcg @ 10 : 0.200769, 0.140479\n","purchase hr and ndcg @10 : 0.391000, 0.306467\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 15: 7447.000000\n","clicks hr ndcg @ 15 : 0.220143, 0.145605\n","purchase hr and ndcg @15 : 0.411438, 0.311885\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","cumulative reward @ 20: 7853.600000\n","clicks hr ndcg @ 20 : 0.233756, 0.148823\n","purchase hr and ndcg @20 : 0.427214, 0.315599\n","off-line corrected evaluation (click_ng,purchase_ng)@10: 0.025905, 0.075982\n","#############################################################\n","the loss in 16200th batch is: 2.675478\n","the loss in 16400th batch is: 2.398004\n","the loss in 16600th batch is: 2.506625\n","the loss in 16800th batch is: 2.784015\n","the loss in 17000th batch is: 2.733599\n","the loss in 17200th batch is: 2.681861\n"]}],"source":["!python \"/content/recommendation-system/Explore_CQL/DRL2/src/SA2C_vAndrew.py\" --model=SASRec --cql=True --epoch=10 --data='/content/recommendation-system/Explore_CQL/Data/RR_data'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CSJ1AtfNLGu"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"AIPI540","language":"python","name":"aipi540"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}