{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szheng3/recommendation-system/blob/main/Explore_CQL/DLR2/SA2C_Recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7wB1Zhi-UT2"
      },
      "source": [
        "# Self-Attentive Sequential Recommender (SASRec) + Supervised Advantage Actor-Critic (SA2C) Recommender on Retail Rocket Data with CQL loss\n",
        "In this notebook, we train an SASRec-SA2C model to recommend a list of items to users in the dataset. We are using Retail Rocket data to train this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsguFFhG-UT5"
      },
      "source": [
        "1. Clone the git repository containing all the source code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F3JxkBS-UT5",
        "outputId": "3739d530-b963-4b45-f6d1-0091d6afd670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'recommendation-system'...\n",
            "remote: Enumerating objects: 1540, done.\u001b[K\n",
            "remote: Counting objects: 100% (321/321), done.\u001b[K\n",
            "remote: Compressing objects: 100% (153/153), done.\u001b[K\n",
            "remote: Total 1540 (delta 177), reused 291 (delta 168), pack-reused 1219\u001b[K\n",
            "Receiving objects: 100% (1540/1540), 11.66 MiB | 8.90 MiB/s, done.\n",
            "Resolving deltas: 100% (807/807), done.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!git clone https://github.com/szheng3/recommendation-system.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8chGbqtd-UT5"
      },
      "source": [
        "2. Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dQAWWHH-UT6"
      },
      "outputs": [],
      "source": [
        "#!pip install -r '/content/recommendation-system/Explore_CQL/requirements.txt'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trfl d3rlpy"
      ],
      "metadata": {
        "id": "SEwfgEFJshEu",
        "outputId": "305a96e6-37e2-420b-afad-98fcec2aa12c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting trfl\n",
            "  Using cached trfl-1.2.0-py3-none-any.whl (104 kB)\n",
            "Collecting d3rlpy\n",
            "  Downloading d3rlpy-1.1.1.tar.gz (317 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.6/317.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from trfl) (1.22.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from trfl) (1.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from trfl) (1.16.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from trfl) (1.4.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from trfl) (0.1.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from d3rlpy) (2.0.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from d3rlpy) (1.2.2)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from d3rlpy) (4.65.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from d3rlpy) (3.8.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from d3rlpy) (0.25.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from d3rlpy) (8.1.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from d3rlpy) (4.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from d3rlpy) (1.10.1)\n",
            "Collecting structlog\n",
            "  Downloading structlog-23.1.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->d3rlpy) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->d3rlpy) (0.0.8)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->d3rlpy) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->d3rlpy) (3.1.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->d3rlpy) (3.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX->d3rlpy) (23.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->d3rlpy) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->d3rlpy) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->d3rlpy) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->d3rlpy) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->d3rlpy) (3.12.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->d3rlpy) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->d3rlpy) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->d3rlpy) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->d3rlpy) (1.3.0)\n",
            "Building wheels for collected packages: d3rlpy\n",
            "  Building wheel for d3rlpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for d3rlpy: filename=d3rlpy-1.1.1-cp310-cp310-linux_x86_64.whl size=1257801 sha256=aabacabd29073c5fec11938d4516867d90971cd6f3317707dba5c28014f5fc20\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/44/c4/aab790777bd0d1358b9e8cc391621a4e0e2b10e7cd03406ad6\n",
            "Successfully built d3rlpy\n",
            "Installing collected packages: trfl, tensorboardX, structlog, colorama, d3rlpy\n",
            "Successfully installed colorama-0.4.6 d3rlpy-1.1.1 structlog-23.1.0 tensorboardX-2.6 trfl-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0PXshJm-UT6"
      },
      "source": [
        "3. Download Retail Rocket events log from AWS S3 bucket to local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5xW9F1D-UT6",
        "outputId": "cbcce60f-4a53-420d-9f49-e3aea400b485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-03 09:52:00--  https://aipi590.s3.amazonaws.com/events.csv\n",
            "Resolving aipi590.s3.amazonaws.com (aipi590.s3.amazonaws.com)... 3.5.7.110, 52.216.40.177, 52.217.227.225, ...\n",
            "Connecting to aipi590.s3.amazonaws.com (aipi590.s3.amazonaws.com)|3.5.7.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94237913 (90M) [text/csv]\n",
            "Saving to: ‘/content/recommendation-system/Explore_CQL/Data/RR_data/events.csv’\n",
            "\n",
            "events.csv          100%[===================>]  89.87M  49.1MB/s    in 1.8s    \n",
            "\n",
            "2023-05-03 09:52:02 (49.1 MB/s) - ‘/content/recommendation-system/Explore_CQL/Data/RR_data/events.csv’ saved [94237913/94237913]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://aipi590.s3.amazonaws.com/events.csv -P '/content/recommendation-system/Explore_CQL/Data/RR_data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py81qAwA-UT6"
      },
      "source": [
        "4. Run script to pre-process data and generate replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TprxYm5w-UT7",
        "outputId": "0a33692c-025d-40d2-a786-7ef2d521861c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-03 09:52:11.120535: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-05-03 09:52:11.176938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-03 09:52:12.112583: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "Starting to pre-process data...\n",
            "\n",
            "Sorting and pickling data...\n",
            "\n",
            "Splitting data into train, validation, and test sets...\n",
            "\n",
            "Pickling train, validation, and test sets...\n",
            "\n",
            "Calculating item popularity and storing as dictionary...\n",
            "\n",
            "Generating replay buffer from train set...\n",
            "\n",
            "Pickling replay buffer...\n",
            "\n",
            "Pickling data statistics...\n",
            "\n",
            "Script completed successfully!\n"
          ]
        }
      ],
      "source": [
        "!python '/content/recommendation-system/Explore_CQL/DLR2/src/gen_replay_buffer.py' --data='/content/recommendation-system/Explore_CQL/Data/RR_data'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHsaKA_e-UT7"
      },
      "source": [
        "5. Run script to begin training and evaluate model. The model below is SASRec-SA2C. This run is without CQL. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jjqlOt--UT7",
        "outputId": "624e1ea5-5368-45ab-f252-97250e1c8040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-03 10:01:14.500302: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-05-03 10:01:14.553825: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-03 10:01:15.577763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Not using CQL loss\n",
            "2023-05-03 10:01:17.536452: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-03 10:01:18.035394: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-03 10:01:18.091689: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-03 10:01:18.131998: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "Not using CQL loss\n",
            "2023-05-03 10:01:20.987414: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-03 10:01:21.144286: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-03 10:01:21.204494: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-03 10:01:21.247976: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-03 10:01:30.011878: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-03 10:01:30.011989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38286 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n",
            "2023-05-03 10:01:30.107039: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
            "epoch 1\n",
            "2023-05-03 10:01:35.183632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
            "2023-05-03 10:01:37.264037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
            "the loss in 200th batch is: 10.970873\n",
            "the loss in 400th batch is: 10.723507\n",
            "the loss in 600th batch is: 10.617424\n",
            "the loss in 800th batch is: 10.514730\n",
            "the loss in 1000th batch is: 10.261164\n",
            "the loss in 1200th batch is: 10.141164\n",
            "the loss in 1400th batch is: 9.812000\n",
            "the loss in 1600th batch is: 10.350781\n",
            "the loss in 1800th batch is: 9.638980\n",
            "the loss in 2000th batch is: 9.575159\n",
            "the loss in 2200th batch is: 9.217543\n",
            "the loss in 2400th batch is: 9.772926\n",
            "the loss in 2600th batch is: 9.670841\n",
            "the loss in 2800th batch is: 8.925387\n",
            "the loss in 3000th batch is: 9.220270\n",
            "the loss in 3200th batch is: 9.172510\n",
            "the loss in 3400th batch is: 9.046860\n",
            "the loss in 3600th batch is: 8.770576\n",
            "the loss in 3800th batch is: 8.844150\n",
            "epoch 2\n",
            "the loss in 4000th batch is: 8.584906\n",
            "the loss in 4200th batch is: 8.368204\n",
            "the loss in 4400th batch is: 8.524826\n",
            "the loss in 4600th batch is: 8.210434\n",
            "the loss in 4800th batch is: 8.244720\n",
            "the loss in 5000th batch is: 8.280823\n",
            "the loss in 5200th batch is: 8.040416\n",
            "the loss in 5400th batch is: 8.184804\n",
            "the loss in 5600th batch is: 7.838883\n",
            "the loss in 5800th batch is: 7.891706\n",
            "the loss in 6000th batch is: 8.021540\n",
            "the loss in 6200th batch is: 7.573285\n",
            "the loss in 6400th batch is: 7.882727\n",
            "the loss in 6600th batch is: 7.801809\n",
            "the loss in 6800th batch is: 7.631364\n",
            "the loss in 7000th batch is: 7.844499\n",
            "the loss in 7200th batch is: 7.549035\n",
            "the loss in 7400th batch is: 7.331873\n",
            "the loss in 7600th batch is: 7.295918\n",
            "epoch 3\n",
            "the loss in 7800th batch is: 7.352780\n",
            "the loss in 8000th batch is: 7.474254\n",
            "\n",
            "Beginning evaluation...\n",
            "#############################################################\n",
            "total clicks: 117015, total purchase:5578\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 8493.200000\n",
            "clicks hr ndcg @ 5 : 0.243012, 0.190637\n",
            "purchase hr and ndcg @5 : 0.503048, 0.429984\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 9840.000000\n",
            "clicks hr ndcg @ 10 : 0.289066, 0.205572\n",
            "purchase hr and ndcg @10 : 0.551273, 0.445745\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 10536.800000\n",
            "clicks hr ndcg @ 15 : 0.313626, 0.212080\n",
            "purchase hr and ndcg @15 : 0.573144, 0.451561\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 11007.400000\n",
            "clicks hr ndcg @ 20 : 0.330317, 0.216024\n",
            "purchase hr and ndcg @20 : 0.587487, 0.454950\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.058889, 0.166229\n",
            "#############################################################\n",
            "the loss in 8200th batch is: 7.397746\n",
            "the loss in 8400th batch is: 7.242693\n",
            "the loss in 8600th batch is: 7.049873\n",
            "the loss in 8800th batch is: 7.405783\n",
            "the loss in 9000th batch is: 6.833628\n",
            "the loss in 9200th batch is: 6.795839\n",
            "the loss in 9400th batch is: 6.707775\n",
            "the loss in 9600th batch is: 7.026587\n",
            "the loss in 9800th batch is: 6.837717\n",
            "the loss in 10000th batch is: 6.979034\n",
            "the loss in 10200th batch is: 6.534665\n",
            "the loss in 10400th batch is: 7.343348\n",
            "the loss in 10600th batch is: 6.782298\n",
            "the loss in 10800th batch is: 6.673348\n",
            "the loss in 11000th batch is: 6.749410\n",
            "the loss in 11200th batch is: 6.532672\n",
            "the loss in 11400th batch is: 6.376647\n",
            "epoch 4\n",
            "the loss in 11600th batch is: 6.389229\n",
            "the loss in 11800th batch is: 6.550349\n",
            "the loss in 12000th batch is: 6.317048\n",
            "the loss in 12200th batch is: 5.961470\n",
            "the loss in 12400th batch is: 6.657063\n",
            "the loss in 12600th batch is: 6.818182\n",
            "the loss in 12800th batch is: 5.962524\n",
            "the loss in 13000th batch is: 6.351500\n",
            "the loss in 13200th batch is: 6.132376\n",
            "the loss in 13400th batch is: 6.256855\n",
            "the loss in 13600th batch is: 6.500292\n",
            "the loss in 13800th batch is: 6.268919\n",
            "the loss in 14000th batch is: 6.078899\n",
            "the loss in 14200th batch is: 6.498261\n",
            "the loss in 14400th batch is: 6.431740\n",
            "the loss in 14600th batch is: 6.157998\n",
            "the loss in 14800th batch is: 6.603189\n",
            "the loss in 15000th batch is: 6.382616\n",
            "the loss in 15200th batch is: 1.136344\n",
            "epoch 5\n",
            "the loss in 15400th batch is: 1.148381\n",
            "the loss in 15600th batch is: 1.203015\n",
            "the loss in 15800th batch is: 1.226399\n",
            "the loss in 16000th batch is: 1.160913\n",
            "the loss in 16200th batch is: 1.037253\n",
            "the loss in 16400th batch is: 1.047176\n",
            "the loss in 16600th batch is: 0.998933\n",
            "the loss in 16800th batch is: 1.068031\n",
            "the loss in 17000th batch is: 0.997468\n",
            "the loss in 17200th batch is: 1.048592\n",
            "the loss in 17400th batch is: 1.106600\n",
            "the loss in 17600th batch is: 1.017261\n",
            "the loss in 17800th batch is: 0.973838\n",
            "the loss in 18000th batch is: 0.850742\n",
            "the loss in 18200th batch is: 1.064899\n",
            "the loss in 18400th batch is: 1.035540\n",
            "the loss in 18600th batch is: 0.946810\n",
            "the loss in 18800th batch is: 1.031567\n",
            "the loss in 19000th batch is: 0.988524\n",
            "the loss in 19200th batch is: 1.074340\n",
            "epoch 6\n",
            "the loss in 19400th batch is: 1.003167\n",
            "the loss in 19600th batch is: 0.994567\n",
            "the loss in 19800th batch is: 0.924037\n",
            "the loss in 20000th batch is: 0.970776\n",
            "the loss in 20200th batch is: 0.947763\n",
            "the loss in 20400th batch is: 0.928322\n",
            "the loss in 20600th batch is: 1.075919\n",
            "the loss in 20800th batch is: 0.885843\n",
            "the loss in 21000th batch is: 0.862916\n",
            "the loss in 21200th batch is: 0.897888\n",
            "the loss in 21400th batch is: 0.827976\n",
            "the loss in 21600th batch is: 0.864486\n",
            "the loss in 21800th batch is: 0.932525\n",
            "the loss in 22000th batch is: 0.968620\n",
            "the loss in 22200th batch is: 0.877246\n",
            "the loss in 22400th batch is: 0.861909\n",
            "the loss in 22600th batch is: 0.948664\n",
            "the loss in 22800th batch is: 0.912831\n",
            "the loss in 23000th batch is: 0.782394\n",
            "epoch 7\n",
            "the loss in 23200th batch is: 0.807030\n",
            "the loss in 23400th batch is: 0.728297\n",
            "the loss in 23600th batch is: 0.834684\n",
            "the loss in 23800th batch is: 0.884413\n",
            "the loss in 24000th batch is: 0.835491\n",
            "the loss in 24200th batch is: 0.876078\n",
            "the loss in 24400th batch is: 0.839829\n",
            "the loss in 24600th batch is: 0.750272\n",
            "the loss in 24800th batch is: 0.845600\n",
            "the loss in 25000th batch is: 0.740140\n",
            "the loss in 25200th batch is: 0.754520\n",
            "the loss in 25400th batch is: 0.882258\n",
            "the loss in 25600th batch is: 0.765518\n",
            "the loss in 25800th batch is: 0.767861\n",
            "the loss in 26000th batch is: 0.806848\n",
            "the loss in 26200th batch is: 0.779751\n",
            "the loss in 26400th batch is: 0.748052\n",
            "the loss in 26600th batch is: 0.791004\n",
            "the loss in 26800th batch is: 0.808176\n",
            "epoch 8\n",
            "the loss in 27000th batch is: 0.769019\n",
            "the loss in 27200th batch is: 0.698980\n",
            "the loss in 27400th batch is: 0.766872\n",
            "the loss in 27600th batch is: 0.797347\n",
            "the loss in 27800th batch is: 0.666490\n",
            "the loss in 28000th batch is: 0.748204\n",
            "the loss in 28200th batch is: 0.716759\n",
            "the loss in 28400th batch is: 0.651247\n",
            "the loss in 28600th batch is: 0.730325\n",
            "the loss in 28800th batch is: 0.668516\n",
            "the loss in 29000th batch is: 0.723908\n",
            "the loss in 29200th batch is: 0.692442\n",
            "the loss in 29400th batch is: 0.700702\n",
            "the loss in 29600th batch is: 0.766133\n",
            "the loss in 29800th batch is: 0.679919\n",
            "the loss in 30000th batch is: 0.644693\n",
            "the loss in 30200th batch is: 0.576509\n",
            "the loss in 30400th batch is: 0.759408\n",
            "the loss in 30600th batch is: 0.730242\n",
            "epoch 9\n",
            "the loss in 30800th batch is: 0.647402\n",
            "the loss in 31000th batch is: 0.680790\n",
            "the loss in 31200th batch is: 0.674710\n",
            "the loss in 31400th batch is: 0.605360\n",
            "the loss in 31600th batch is: 0.665347\n",
            "the loss in 31800th batch is: 0.731118\n",
            "the loss in 32000th batch is: 0.567393\n",
            "the loss in 32200th batch is: 0.726344\n",
            "the loss in 32400th batch is: 0.661863\n",
            "the loss in 32600th batch is: 0.531629\n",
            "the loss in 32800th batch is: 0.674205\n",
            "the loss in 33000th batch is: 0.669361\n",
            "the loss in 33200th batch is: 0.672249\n",
            "the loss in 33400th batch is: 0.636391\n",
            "the loss in 33600th batch is: 0.570948\n",
            "the loss in 33800th batch is: 0.675025\n",
            "the loss in 34000th batch is: 0.740596\n",
            "the loss in 34200th batch is: 0.726171\n",
            "the loss in 34400th batch is: 0.627365\n",
            "the loss in 34600th batch is: 0.675616\n",
            "epoch 10\n",
            "the loss in 34800th batch is: 0.671160\n",
            "the loss in 35000th batch is: 0.612903\n",
            "the loss in 35200th batch is: 0.650212\n",
            "the loss in 35400th batch is: 0.655074\n",
            "the loss in 35600th batch is: 0.620647\n",
            "the loss in 35800th batch is: 0.601978\n",
            "the loss in 36000th batch is: 0.651226\n",
            "the loss in 36200th batch is: 0.631803\n",
            "the loss in 36400th batch is: 0.682962\n",
            "the loss in 36600th batch is: 0.644826\n",
            "the loss in 36800th batch is: 0.647305\n",
            "the loss in 37000th batch is: 0.668343\n",
            "the loss in 37200th batch is: 0.667361\n",
            "the loss in 37400th batch is: 0.589978\n",
            "the loss in 37600th batch is: 0.614029\n",
            "the loss in 37800th batch is: 0.586492\n",
            "the loss in 38000th batch is: 0.682008\n",
            "the loss in 38200th batch is: 0.546054\n",
            "the loss in 38400th batch is: 0.591875\n",
            "Training completed...\n",
            "Evaluating test dataset...\n",
            "\n",
            "#############################################################\n",
            "total clicks: 120730, total purchase:6190\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 10512.600000\n",
            "clicks hr ndcg @ 5 : 0.284006, 0.223211\n",
            "purchase hr and ndcg @5 : 0.590468, 0.514403\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 12030.800000\n",
            "clicks hr ndcg @ 10 : 0.335078, 0.239783\n",
            "purchase hr and ndcg @10 : 0.636511, 0.529463\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 12803.200000\n",
            "clicks hr ndcg @ 15 : 0.362718, 0.247114\n",
            "purchase hr and ndcg @15 : 0.653473, 0.533920\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 13334.600000\n",
            "clicks hr ndcg @ 20 : 0.381827, 0.251628\n",
            "purchase hr and ndcg @20 : 0.664782, 0.536581\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.084990, 0.239115\n",
            "#############################################################\n"
          ]
        }
      ],
      "source": [
        "!python \"/content/recommendation-system/Explore_CQL/DLR2/src/SA2C_v3_5.py\" --model=SASRec --epoch=10 --data=\"/content/recommendation-system/Explore_CQL/Data/RR_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5b. Run script to beging training and evaluate model. The model below is SASRec-SA2C. This is run with CQL and alpha factors of 1.0 and 0.5."
      ],
      "metadata": {
        "id": "sEexsJcyaUxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python \"/content/recommendation-system/Explore_CQL/DLR2/src/SA2C_v3_5.py\" --model=SASRec --epoch=10 --CQL_alpha=1.0 --data=\"/content/recommendation-system/Explore_CQL/Data/RR_data\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoLjrJHu_ZOO",
        "outputId": "edee8334-1ac9-47d6-fc26-445b75400f1b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-03 11:46:30.671475: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-05-03 11:46:30.724254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-03 11:46:31.745776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Using CQL loss.\n",
            "2023-05-03 11:46:33.629547: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-03 11:46:33.860822: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-03 11:46:33.917933: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-03 11:46:33.958496: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "Using CQL loss.\n",
            "2023-05-03 11:46:36.997575: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-03 11:46:37.156008: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-03 11:46:37.215537: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-03 11:46:37.260182: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-03 11:46:44.797441: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-03 11:46:44.797499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38286 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n",
            "2023-05-03 11:46:44.893819: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
            "epoch 1\n",
            "2023-05-03 11:46:47.126986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
            "2023-05-03 11:46:48.559959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
            "the loss in 200th batch is: 23.077339\n",
            "the loss in 400th batch is: 22.427135\n",
            "the loss in 600th batch is: 22.101751\n",
            "the loss in 800th batch is: 22.197197\n",
            "the loss in 1000th batch is: 22.301147\n",
            "the loss in 1200th batch is: 22.002262\n",
            "the loss in 1400th batch is: 21.923775\n",
            "the loss in 1600th batch is: 21.678375\n",
            "the loss in 1800th batch is: 21.878479\n",
            "the loss in 2000th batch is: 21.540989\n",
            "the loss in 2200th batch is: 21.460159\n",
            "the loss in 2400th batch is: 21.460611\n",
            "the loss in 2600th batch is: 21.232206\n",
            "the loss in 2800th batch is: 21.307991\n",
            "the loss in 3000th batch is: 20.964808\n",
            "the loss in 3200th batch is: 21.035025\n",
            "the loss in 3400th batch is: 20.701406\n",
            "the loss in 3600th batch is: 21.110863\n",
            "the loss in 3800th batch is: 20.400059\n",
            "epoch 2\n",
            "the loss in 4000th batch is: 20.626173\n",
            "the loss in 4200th batch is: 20.372120\n",
            "the loss in 4400th batch is: 20.587818\n",
            "the loss in 4600th batch is: 20.274714\n",
            "the loss in 4800th batch is: 20.685957\n",
            "the loss in 5000th batch is: 20.578463\n",
            "the loss in 5200th batch is: 19.980642\n",
            "the loss in 5400th batch is: 20.037956\n",
            "the loss in 5600th batch is: 19.904493\n",
            "the loss in 5800th batch is: 19.879700\n",
            "the loss in 6000th batch is: 19.568432\n",
            "the loss in 6200th batch is: 19.854357\n",
            "the loss in 6400th batch is: 20.064552\n",
            "the loss in 6600th batch is: 19.663403\n",
            "the loss in 6800th batch is: 19.760456\n",
            "the loss in 7000th batch is: 19.477531\n",
            "the loss in 7200th batch is: 19.491037\n",
            "the loss in 7400th batch is: 20.173178\n",
            "the loss in 7600th batch is: 19.609707\n",
            "epoch 3\n",
            "the loss in 7800th batch is: 19.387329\n",
            "the loss in 8000th batch is: 19.475975\n",
            "\n",
            "Beginning evaluation...\n",
            "#############################################################\n",
            "total clicks: 117015, total purchase:5578\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 8443.400000\n",
            "clicks hr ndcg @ 5 : 0.241311, 0.189641\n",
            "purchase hr and ndcg @5 : 0.501255, 0.432437\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 9746.000000\n",
            "clicks hr ndcg @ 10 : 0.286587, 0.204359\n",
            "purchase hr and ndcg @10 : 0.544819, 0.446639\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 10450.200000\n",
            "clicks hr ndcg @ 15 : 0.311251, 0.210894\n",
            "purchase hr and ndcg @15 : 0.567587, 0.452702\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 10897.400000\n",
            "clicks hr ndcg @ 20 : 0.326813, 0.214566\n",
            "purchase hr and ndcg @20 : 0.582467, 0.456232\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.058894, 0.165971\n",
            "#############################################################\n",
            "the loss in 8200th batch is: 18.685911\n",
            "the loss in 8400th batch is: 19.462708\n",
            "the loss in 8600th batch is: 19.144211\n",
            "the loss in 8800th batch is: 19.284187\n",
            "the loss in 9000th batch is: 19.061350\n",
            "the loss in 9200th batch is: 18.984350\n",
            "the loss in 9400th batch is: 18.810959\n",
            "the loss in 9600th batch is: 19.068281\n",
            "the loss in 9800th batch is: 18.983957\n",
            "the loss in 10000th batch is: 18.725027\n",
            "the loss in 10200th batch is: 18.699318\n",
            "the loss in 10400th batch is: 19.237894\n",
            "the loss in 10600th batch is: 18.251184\n",
            "the loss in 10800th batch is: 18.652086\n",
            "the loss in 11000th batch is: 19.070580\n",
            "the loss in 11200th batch is: 18.275444\n",
            "the loss in 11400th batch is: 18.793249\n",
            "epoch 4\n",
            "the loss in 11600th batch is: 18.795990\n",
            "the loss in 11800th batch is: 18.738132\n",
            "the loss in 12000th batch is: 18.297718\n",
            "the loss in 12200th batch is: 18.513988\n",
            "the loss in 12400th batch is: 18.572714\n",
            "the loss in 12600th batch is: 18.272224\n",
            "the loss in 12800th batch is: 18.213816\n",
            "the loss in 13000th batch is: 18.577919\n",
            "the loss in 13200th batch is: 18.264023\n",
            "the loss in 13400th batch is: 18.477520\n",
            "the loss in 13600th batch is: 18.311794\n",
            "the loss in 13800th batch is: 17.965475\n",
            "the loss in 14000th batch is: 17.915760\n",
            "the loss in 14200th batch is: 17.672756\n",
            "the loss in 14400th batch is: 18.187195\n",
            "the loss in 14600th batch is: 18.268023\n",
            "the loss in 14800th batch is: 18.150337\n",
            "the loss in 15000th batch is: 18.378960\n",
            "the loss in 15200th batch is: 1.468077\n",
            "epoch 5\n",
            "the loss in 15400th batch is: 1.269920\n",
            "the loss in 15600th batch is: 1.352672\n",
            "the loss in 15800th batch is: 1.395104\n",
            "the loss in 16000th batch is: 1.342656\n",
            "the loss in 16200th batch is: 1.354485\n",
            "the loss in 16400th batch is: 1.277327\n",
            "the loss in 16600th batch is: 1.116543\n",
            "the loss in 16800th batch is: 1.353375\n",
            "the loss in 17000th batch is: 1.066011\n",
            "the loss in 17200th batch is: 1.102723\n",
            "the loss in 17400th batch is: 1.186696\n",
            "the loss in 17600th batch is: 1.100554\n",
            "the loss in 17800th batch is: 1.114215\n",
            "the loss in 18000th batch is: 1.090007\n",
            "the loss in 18200th batch is: 1.125141\n",
            "the loss in 18400th batch is: 1.083200\n",
            "the loss in 18600th batch is: 1.047884\n",
            "the loss in 18800th batch is: 0.987642\n",
            "the loss in 19000th batch is: 0.997086\n",
            "the loss in 19200th batch is: 1.073308\n",
            "epoch 6\n",
            "the loss in 19400th batch is: 1.064095\n",
            "the loss in 19600th batch is: 0.903460\n",
            "the loss in 19800th batch is: 1.032321\n",
            "the loss in 20000th batch is: 0.859536\n",
            "the loss in 20200th batch is: 0.969157\n",
            "the loss in 20400th batch is: 0.861901\n",
            "the loss in 20600th batch is: 0.980431\n",
            "the loss in 20800th batch is: 0.903347\n",
            "the loss in 21000th batch is: 0.927046\n",
            "the loss in 21200th batch is: 0.841246\n",
            "the loss in 21400th batch is: 0.871906\n",
            "the loss in 21600th batch is: 0.833729\n",
            "the loss in 21800th batch is: 0.860296\n",
            "the loss in 22000th batch is: 0.930158\n",
            "the loss in 22200th batch is: 0.844764\n",
            "the loss in 22400th batch is: 0.824458\n",
            "the loss in 22600th batch is: 0.866725\n",
            "the loss in 22800th batch is: 0.807621\n",
            "the loss in 23000th batch is: 0.758742\n",
            "epoch 7\n",
            "the loss in 23200th batch is: 0.694177\n",
            "the loss in 23400th batch is: 0.766973\n",
            "the loss in 23600th batch is: 0.711767\n",
            "the loss in 23800th batch is: 0.848879\n",
            "the loss in 24000th batch is: 0.871099\n",
            "the loss in 24200th batch is: 0.736187\n",
            "the loss in 24400th batch is: 0.844657\n",
            "the loss in 24600th batch is: 0.721164\n",
            "the loss in 24800th batch is: 0.812856\n",
            "the loss in 25000th batch is: 0.720725\n",
            "the loss in 25200th batch is: 0.817545\n",
            "the loss in 25400th batch is: 0.677802\n",
            "the loss in 25600th batch is: 0.758818\n",
            "the loss in 25800th batch is: 0.814141\n",
            "the loss in 26000th batch is: 0.777298\n",
            "the loss in 26200th batch is: 0.716354\n",
            "the loss in 26400th batch is: 0.716077\n",
            "the loss in 26600th batch is: 0.714069\n",
            "the loss in 26800th batch is: 0.731481\n",
            "epoch 8\n",
            "the loss in 27000th batch is: 0.680929\n",
            "the loss in 27200th batch is: 0.726780\n",
            "the loss in 27400th batch is: 0.745269\n",
            "the loss in 27600th batch is: 0.796761\n",
            "the loss in 27800th batch is: 0.684491\n",
            "the loss in 28000th batch is: 0.877434\n",
            "the loss in 28200th batch is: 0.623899\n",
            "the loss in 28400th batch is: 0.673530\n",
            "the loss in 28600th batch is: 0.682195\n",
            "the loss in 28800th batch is: 0.657984\n",
            "the loss in 29000th batch is: 0.648648\n",
            "the loss in 29200th batch is: 0.644219\n",
            "the loss in 29400th batch is: 0.701715\n",
            "the loss in 29600th batch is: 0.643160\n",
            "the loss in 29800th batch is: 0.653744\n",
            "the loss in 30000th batch is: 0.582646\n",
            "the loss in 30200th batch is: 0.646984\n",
            "the loss in 30400th batch is: 0.754276\n",
            "the loss in 30600th batch is: 0.594672\n",
            "epoch 9\n",
            "the loss in 30800th batch is: 0.646508\n",
            "the loss in 31000th batch is: 0.661871\n",
            "the loss in 31200th batch is: 0.608569\n",
            "the loss in 31400th batch is: 0.629198\n",
            "the loss in 31600th batch is: 0.579512\n",
            "the loss in 31800th batch is: 0.707556\n",
            "the loss in 32000th batch is: 0.648276\n",
            "the loss in 32200th batch is: 0.674604\n",
            "the loss in 32400th batch is: 0.607636\n",
            "the loss in 32600th batch is: 0.605053\n",
            "the loss in 32800th batch is: 0.694658\n",
            "the loss in 33000th batch is: 0.663344\n",
            "the loss in 33200th batch is: 0.642143\n",
            "the loss in 33400th batch is: 0.578665\n",
            "the loss in 33600th batch is: 0.466794\n",
            "the loss in 33800th batch is: 0.643529\n",
            "the loss in 34000th batch is: 0.684808\n",
            "the loss in 34200th batch is: 0.616645\n",
            "the loss in 34400th batch is: 0.641772\n",
            "the loss in 34600th batch is: 0.634029\n",
            "epoch 10\n",
            "the loss in 34800th batch is: 0.593739\n",
            "the loss in 35000th batch is: 0.662136\n",
            "the loss in 35200th batch is: 0.664563\n",
            "the loss in 35400th batch is: 0.551071\n",
            "the loss in 35600th batch is: 0.695577\n",
            "the loss in 35800th batch is: 0.601950\n",
            "the loss in 36000th batch is: 0.586468\n",
            "the loss in 36200th batch is: 0.605003\n",
            "the loss in 36400th batch is: 0.570221\n",
            "the loss in 36600th batch is: 0.570110\n",
            "the loss in 36800th batch is: 0.594238\n",
            "the loss in 37000th batch is: 0.631436\n",
            "the loss in 37200th batch is: 0.568042\n",
            "the loss in 37400th batch is: 0.586386\n",
            "the loss in 37600th batch is: 0.616827\n",
            "the loss in 37800th batch is: 0.574003\n",
            "the loss in 38000th batch is: 0.615932\n",
            "the loss in 38200th batch is: 0.538532\n",
            "the loss in 38400th batch is: 0.614182\n",
            "Training completed...\n",
            "Evaluating test dataset...\n",
            "\n",
            "#############################################################\n",
            "total clicks: 120730, total purchase:6190\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 10554.600000\n",
            "clicks hr ndcg @ 5 : 0.286615, 0.225026\n",
            "purchase hr and ndcg @5 : 0.587076, 0.505963\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 12035.600000\n",
            "clicks hr ndcg @ 10 : 0.337679, 0.241563\n",
            "purchase hr and ndcg @10 : 0.627141, 0.519070\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 12861.200000\n",
            "clicks hr ndcg @ 15 : 0.365949, 0.249058\n",
            "purchase hr and ndcg @15 : 0.650242, 0.525233\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 13380.200000\n",
            "clicks hr ndcg @ 20 : 0.384503, 0.253445\n",
            "purchase hr and ndcg @20 : 0.661712, 0.527941\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.089702, 0.239576\n",
            "#############################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python \"/content/recommendation-system/Explore_CQL/DLR2/src/SA2C_v3_5.py\" --model=SASRec --epoch=10 --CQL_alpha=0.5 --data=\"/content/recommendation-system/Explore_CQL/Data/RR_data\""
      ],
      "metadata": {
        "id": "S6Ws5Bwz_oNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c4af60-4f62-40cb-e065-4a25e9b5f88b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-03 13:31:15.838967: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-05-03 13:31:15.891639: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-03 13:31:16.941371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Using CQL loss.\n",
            "2023-05-03 13:31:18.893645: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-03 13:31:19.117940: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-03 13:31:19.176736: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "2023-05-03 13:31:19.219864: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder' with dtype bool\n",
            "\t [[{{node Placeholder}}]]\n",
            "Using CQL loss.\n",
            "2023-05-03 13:31:22.295754: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-03 13:31:22.455363: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-03 13:31:22.515351: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-03 13:31:22.560697: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder_1' with dtype bool\n",
            "\t [[{{node Placeholder_1}}]]\n",
            "2023-05-03 13:31:30.191508: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2023-05-03 13:31:30.191591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38286 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n",
            "2023-05-03 13:31:30.291636: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
            "epoch 1\n",
            "2023-05-03 13:31:32.564827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
            "2023-05-03 13:31:34.013026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
            "the loss in 200th batch is: 36.253876\n",
            "the loss in 400th batch is: 35.545055\n",
            "the loss in 600th batch is: 34.897572\n",
            "the loss in 800th batch is: 34.888279\n",
            "the loss in 1000th batch is: 35.490292\n",
            "the loss in 1200th batch is: 35.336990\n",
            "the loss in 1400th batch is: 35.174011\n",
            "the loss in 1600th batch is: 35.193150\n",
            "the loss in 1800th batch is: 35.036774\n",
            "the loss in 2000th batch is: 34.789764\n",
            "the loss in 2200th batch is: 35.091137\n",
            "the loss in 2400th batch is: 34.762814\n",
            "the loss in 2600th batch is: 34.639713\n",
            "the loss in 2800th batch is: 34.682049\n",
            "the loss in 3000th batch is: 34.573708\n",
            "the loss in 3200th batch is: 34.205452\n",
            "the loss in 3400th batch is: 34.621704\n",
            "the loss in 3600th batch is: 33.620262\n",
            "the loss in 3800th batch is: 34.120430\n",
            "epoch 2\n",
            "the loss in 4000th batch is: 34.224060\n",
            "the loss in 4200th batch is: 34.430676\n",
            "the loss in 4400th batch is: 33.734718\n",
            "the loss in 4600th batch is: 33.482265\n",
            "the loss in 4800th batch is: 33.766312\n",
            "the loss in 5000th batch is: 33.528217\n",
            "the loss in 5200th batch is: 33.774490\n",
            "the loss in 5400th batch is: 33.522934\n",
            "the loss in 5600th batch is: 33.037861\n",
            "the loss in 5800th batch is: 33.263115\n",
            "the loss in 6000th batch is: 32.588562\n",
            "the loss in 6200th batch is: 33.026440\n",
            "the loss in 6400th batch is: 33.064297\n",
            "the loss in 6600th batch is: 33.013474\n",
            "the loss in 6800th batch is: 32.760807\n",
            "the loss in 7000th batch is: 32.675346\n",
            "the loss in 7200th batch is: 32.260010\n",
            "the loss in 7400th batch is: 32.395805\n",
            "the loss in 7600th batch is: 32.886456\n",
            "epoch 3\n",
            "the loss in 7800th batch is: 32.927731\n",
            "the loss in 8000th batch is: 32.942047\n",
            "\n",
            "Beginning evaluation...\n",
            "#############################################################\n",
            "total clicks: 117015, total purchase:5578\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 8423.800000\n",
            "clicks hr ndcg @ 5 : 0.241285, 0.188970\n",
            "purchase hr and ndcg @5 : 0.497849, 0.426941\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 9689.400000\n",
            "clicks hr ndcg @ 10 : 0.284553, 0.202971\n",
            "purchase hr and ndcg @10 : 0.543205, 0.441744\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 10394.800000\n",
            "clicks hr ndcg @ 15 : 0.309183, 0.209492\n",
            "purchase hr and ndcg @15 : 0.566332, 0.447863\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 10878.600000\n",
            "clicks hr ndcg @ 20 : 0.325753, 0.213414\n",
            "purchase hr and ndcg @20 : 0.583542, 0.451948\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.058886, 0.168113\n",
            "#############################################################\n",
            "the loss in 8200th batch is: 32.528755\n",
            "the loss in 8400th batch is: 32.680897\n",
            "the loss in 8600th batch is: 32.588715\n",
            "the loss in 8800th batch is: 32.671722\n",
            "the loss in 9000th batch is: 32.300934\n",
            "the loss in 9200th batch is: 32.401871\n",
            "the loss in 9400th batch is: 32.125198\n",
            "the loss in 9600th batch is: 32.013348\n",
            "the loss in 9800th batch is: 31.961264\n",
            "the loss in 10000th batch is: 31.773741\n",
            "the loss in 10200th batch is: 32.198189\n",
            "the loss in 10400th batch is: 31.946951\n",
            "the loss in 10600th batch is: 32.284664\n",
            "the loss in 10800th batch is: 31.505722\n",
            "the loss in 11000th batch is: 32.048534\n",
            "the loss in 11200th batch is: 32.044647\n",
            "the loss in 11400th batch is: 31.902975\n",
            "epoch 4\n",
            "the loss in 11600th batch is: 32.014587\n",
            "the loss in 11800th batch is: 31.569574\n",
            "the loss in 12000th batch is: 32.324257\n",
            "the loss in 12200th batch is: 31.441494\n",
            "the loss in 12400th batch is: 31.637493\n",
            "the loss in 12600th batch is: 31.672297\n",
            "the loss in 12800th batch is: 32.078728\n",
            "the loss in 13000th batch is: 31.263866\n",
            "the loss in 13200th batch is: 31.492653\n",
            "the loss in 13400th batch is: 31.564419\n",
            "the loss in 13600th batch is: 31.904102\n",
            "the loss in 13800th batch is: 31.723833\n",
            "the loss in 14000th batch is: 31.290670\n",
            "the loss in 14200th batch is: 31.676849\n",
            "the loss in 14400th batch is: 31.053217\n",
            "the loss in 14600th batch is: 31.654490\n",
            "the loss in 14800th batch is: 30.904564\n",
            "the loss in 15000th batch is: 31.487862\n",
            "the loss in 15200th batch is: 1.528988\n",
            "epoch 5\n",
            "the loss in 15400th batch is: 1.605293\n",
            "the loss in 15600th batch is: 1.527122\n",
            "the loss in 15800th batch is: 1.443599\n",
            "the loss in 16000th batch is: 1.390777\n",
            "the loss in 16200th batch is: 1.307511\n",
            "the loss in 16400th batch is: 1.569063\n",
            "the loss in 16600th batch is: 1.342688\n",
            "the loss in 16800th batch is: 1.140877\n",
            "the loss in 17000th batch is: 1.241298\n",
            "the loss in 17200th batch is: 1.148538\n",
            "the loss in 17400th batch is: 1.176810\n",
            "the loss in 17600th batch is: 1.127292\n",
            "the loss in 17800th batch is: 1.124046\n",
            "the loss in 18000th batch is: 1.084582\n",
            "the loss in 18200th batch is: 1.161949\n",
            "the loss in 18400th batch is: 1.228256\n",
            "the loss in 18600th batch is: 1.143439\n",
            "the loss in 18800th batch is: 1.152899\n",
            "the loss in 19000th batch is: 1.042745\n",
            "the loss in 19200th batch is: 1.099794\n",
            "epoch 6\n",
            "the loss in 19400th batch is: 1.039757\n",
            "the loss in 19600th batch is: 1.015652\n",
            "the loss in 19800th batch is: 0.959336\n",
            "the loss in 20000th batch is: 1.130764\n",
            "the loss in 20200th batch is: 0.931270\n",
            "the loss in 20400th batch is: 0.931633\n",
            "the loss in 20600th batch is: 0.991336\n",
            "the loss in 20800th batch is: 0.948119\n",
            "the loss in 21000th batch is: 0.975102\n",
            "the loss in 21200th batch is: 0.982519\n",
            "the loss in 21400th batch is: 1.010592\n",
            "the loss in 21600th batch is: 0.961711\n",
            "the loss in 21800th batch is: 0.801416\n",
            "the loss in 22000th batch is: 0.915903\n",
            "the loss in 22200th batch is: 0.902243\n",
            "the loss in 22400th batch is: 0.789575\n",
            "the loss in 22600th batch is: 0.852072\n",
            "the loss in 22800th batch is: 0.870976\n",
            "the loss in 23000th batch is: 0.814408\n",
            "epoch 7\n",
            "the loss in 23200th batch is: 0.775060\n",
            "the loss in 23400th batch is: 0.835975\n",
            "the loss in 23600th batch is: 0.894757\n",
            "the loss in 23800th batch is: 0.854962\n",
            "the loss in 24000th batch is: 0.774989\n",
            "the loss in 24200th batch is: 0.731232\n",
            "the loss in 24400th batch is: 0.885773\n",
            "the loss in 24600th batch is: 0.832620\n",
            "the loss in 24800th batch is: 0.703655\n",
            "the loss in 25000th batch is: 0.771403\n",
            "the loss in 25200th batch is: 0.696692\n",
            "the loss in 25400th batch is: 0.783660\n",
            "the loss in 25600th batch is: 0.739624\n",
            "the loss in 25800th batch is: 0.736199\n",
            "the loss in 26000th batch is: 0.800025\n",
            "the loss in 26200th batch is: 0.750189\n",
            "the loss in 26400th batch is: 0.705424\n",
            "the loss in 26600th batch is: 0.693773\n",
            "the loss in 26800th batch is: 0.754074\n",
            "epoch 8\n",
            "the loss in 27000th batch is: 0.746978\n",
            "the loss in 27200th batch is: 0.663878\n",
            "the loss in 27400th batch is: 0.661545\n",
            "the loss in 27600th batch is: 0.688212\n",
            "the loss in 27800th batch is: 0.670389\n",
            "the loss in 28000th batch is: 0.681997\n",
            "the loss in 28200th batch is: 0.776089\n",
            "the loss in 28400th batch is: 0.721332\n",
            "the loss in 28600th batch is: 0.755082\n",
            "the loss in 28800th batch is: 0.575400\n",
            "the loss in 29000th batch is: 0.712679\n",
            "the loss in 29200th batch is: 0.624245\n",
            "the loss in 29400th batch is: 0.648739\n",
            "the loss in 29600th batch is: 0.664704\n",
            "the loss in 29800th batch is: 0.669273\n",
            "the loss in 30000th batch is: 0.643487\n",
            "the loss in 30200th batch is: 0.663250\n",
            "the loss in 30400th batch is: 0.715473\n",
            "the loss in 30600th batch is: 0.735660\n",
            "epoch 9\n",
            "the loss in 30800th batch is: 0.644226\n",
            "the loss in 31000th batch is: 0.642600\n",
            "the loss in 31200th batch is: 0.646265\n",
            "the loss in 31400th batch is: 0.598306\n",
            "the loss in 31600th batch is: 0.670745\n",
            "the loss in 31800th batch is: 0.656438\n",
            "the loss in 32000th batch is: 0.599235\n",
            "the loss in 32200th batch is: 0.646814\n",
            "the loss in 32400th batch is: 0.616797\n",
            "the loss in 32600th batch is: 0.691217\n",
            "the loss in 32800th batch is: 0.669059\n",
            "the loss in 33000th batch is: 0.622040\n",
            "the loss in 33200th batch is: 0.615489\n",
            "the loss in 33400th batch is: 0.610264\n",
            "the loss in 33600th batch is: 0.642088\n",
            "the loss in 33800th batch is: 0.643253\n",
            "the loss in 34000th batch is: 0.653589\n",
            "the loss in 34200th batch is: 0.588338\n",
            "the loss in 34400th batch is: 0.627863\n",
            "the loss in 34600th batch is: 0.595449\n",
            "epoch 10\n",
            "the loss in 34800th batch is: 0.596826\n",
            "the loss in 35000th batch is: 0.582568\n",
            "the loss in 35200th batch is: 0.622384\n",
            "the loss in 35400th batch is: 0.627779\n",
            "the loss in 35600th batch is: 0.610493\n",
            "the loss in 35800th batch is: 0.607590\n",
            "the loss in 36000th batch is: 0.586185\n",
            "the loss in 36200th batch is: 0.613509\n",
            "the loss in 36400th batch is: 0.617312\n",
            "the loss in 36600th batch is: 0.546791\n",
            "the loss in 36800th batch is: 0.572237\n",
            "the loss in 37000th batch is: 0.616768\n",
            "the loss in 37200th batch is: 0.619603\n",
            "the loss in 37400th batch is: 0.587239\n",
            "the loss in 37600th batch is: 0.553888\n",
            "the loss in 37800th batch is: 0.592181\n",
            "the loss in 38000th batch is: 0.659550\n",
            "the loss in 38200th batch is: 0.603065\n",
            "the loss in 38400th batch is: 0.575758\n",
            "Training completed...\n",
            "Evaluating test dataset...\n",
            "\n",
            "#############################################################\n",
            "total clicks: 120730, total purchase:6190\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 5: 10574.400000\n",
            "clicks hr ndcg @ 5 : 0.287186, 0.225906\n",
            "purchase hr and ndcg @5 : 0.588045, 0.508615\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 10: 12095.000000\n",
            "clicks hr ndcg @ 10 : 0.338690, 0.242641\n",
            "purchase hr and ndcg @10 : 0.632795, 0.523089\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 15: 12891.600000\n",
            "clicks hr ndcg @ 15 : 0.366669, 0.250052\n",
            "purchase hr and ndcg @15 : 0.652342, 0.528241\n",
            "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "cumulative reward @ 20: 13445.400000\n",
            "clicks hr ndcg @ 20 : 0.385795, 0.254570\n",
            "purchase hr and ndcg @20 : 0.667205, 0.531737\n",
            "off-line corrected evaluation (click_ng,purchase_ng)@10: 0.091047, 0.241756\n",
            "#############################################################\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
